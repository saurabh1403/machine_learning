{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-59ab05e21164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegressionLearner:\n",
    "    num_coeff = 1      # this doesn't include the bias term\n",
    "    weights = np.random.random(10)\n",
    "    learning_rate = 0.005\n",
    "    convergence_limit = 0.01\n",
    "    converged_batch = 0\n",
    "    converged_batch_limit = 50    # if this many batches are converged, then we can stop training\n",
    "\n",
    "    def __init__(self, num_coeff, alpha):\n",
    "        self.num_coeff = num_coeff\n",
    "        self.learning_rate = alpha\n",
    "        self.weights = np.random.normal(0, 1, (num_coeff + 1, 1))\n",
    "    \n",
    "    def printSummary(self):\n",
    "        print(\"The learnt weights are: \", self.weights)\n",
    "\n",
    "    # return whether the batch converged or not\n",
    "    def learnGradient(self, training_batch, training_labels):\n",
    "        num_rows = training_batch.shape[0]\n",
    "\n",
    "        # few checks on the input\n",
    "        if training_batch.shape[1] != self.num_coeff:\n",
    "            print(\"Invalid number of columns: \", num_coeff)\n",
    "            return -1\n",
    "\n",
    "        # add the first column for bias learning\n",
    "#         print(\"training batch: \", training_batch)\n",
    "        bias_col = np.ones((num_rows, 1))\n",
    "        mod_train_batch = np.append(bias_col, training_batch, axis=1)\n",
    "#         print(\"modified training batch: \", mod_train_batch)\n",
    "\n",
    "        pred_label = np.matmul(mod_train_batch, self.weights)\n",
    "#         print(\"weights are:\", self.weights)\n",
    "#         print(\"predicted label is:\", pred_label)\n",
    "#         print(\"actual label is:\", training_label)\n",
    "        \n",
    "        pred_delta = pred_label - training_labels\n",
    "#         print(\"pred delta is:\", pred_delta)\n",
    "        \n",
    "        max_delta = 0\n",
    "        for i in range(self.num_coeff + 1):\n",
    "            col_feature = np.reshape(mod_train_batch[:, i], (num_rows, 1))\n",
    "#             print(\"col feature is:\", col_feature)\n",
    "            error_delta = pred_delta * col_feature\n",
    "#             print(\"err delta is:\", error_delta)\n",
    "            \n",
    "            dl_by_dw = np.mean(error_delta)\n",
    "            self.weights[i] -= self.learning_rate * dl_by_dw\n",
    "            \n",
    "#             print(\"new weights are:\", self.weights)\n",
    "            max_delta = max(max_delta, np.abs(dl_by_dw))\n",
    "            \n",
    "        if max_delta <= self.convergence_limit:\n",
    "            self.converged_batch += 1\n",
    "            return 1\n",
    "\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epochs passes:  50000\n",
      "The learnt weights are:  [[ -9.96472579]\n",
      " [132.45014674]]\n",
      "Total epochs passes:  100000\n",
      "The learnt weights are:  [[-11.45784358]\n",
      " [132.40661768]]\n",
      "Total epochs passes:  150000\n",
      "The learnt weights are:  [[-11.47864124]\n",
      " [132.2321783 ]]\n",
      "Total epochs passes:  200000\n",
      "The learnt weights are:  [[-11.5042659 ]\n",
      " [131.83512445]]\n",
      "Total epochs passes:  250000\n",
      "The learnt weights are:  [[-11.54591136]\n",
      " [132.68387424]]\n",
      "Total epochs passes:  300000\n",
      "The learnt weights are:  [[-11.4669321 ]\n",
      " [132.92092085]]\n",
      "Total epochs passes:  350000\n",
      "The learnt weights are:  [[-11.40431871]\n",
      " [132.55511229]]\n",
      "Total epochs passes:  400000\n",
      "The learnt weights are:  [[-11.47197556]\n",
      " [132.6485616 ]]\n",
      "Total epochs passes:  450000\n",
      "The learnt weights are:  [[-11.68323434]\n",
      " [131.52526582]]\n",
      "Total epochs passes:  500000\n",
      "The learnt weights are:  [[-10.53727689]\n",
      " [132.74924702]]\n",
      "the learner has converged in  500001\n",
      "The learnt weights are:  [[-10.57447262]\n",
      " [132.23377699]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate the training data for a simple mathematical equation\n",
    "\n",
    "def generate_training_batch(num_coeff, batch_size):    # this doesn't include the bias term\n",
    "    # this is now generating features normalized around 0. \n",
    "    # We would play with it to see how batch norm can handle any feature distribution and scaling aspects\n",
    "    return np.random.normal(10, 10, (batch_size, num_coeff))\n",
    "\n",
    "\n",
    "def generate_training_labels(training_batch):\n",
    "    # y = -11.52 + 132.54 * x1 - 89.23 * x2\n",
    "    coeff = np.array([[132.54]])\n",
    "#     coeff = np.array([[132.54], [-89.23]])\n",
    "    label = np.matmul(training_batch, coeff)\n",
    "    label = label - 11.52\n",
    "    noise = np.random.normal(0, 5, (training_batch.shape[0], 1))\n",
    "    label += noise\n",
    "    return np.reshape(label, (training_batch.shape[0], 1))\n",
    "\n",
    "\n",
    "num_coeff = 1\n",
    "converged_batch = 0\n",
    "batch_size = 100\n",
    "total_batches = 0\n",
    "learner = LinRegressionLearner(num_coeff, 0.005)\n",
    "\n",
    "while(True):\n",
    "    training_batch = generate_training_batch(num_coeff=num_coeff, batch_size=batch_size)\n",
    "    training_label = generate_training_labels(training_batch)\n",
    "    total_batches += 1\n",
    "    if learner.learnGradient(training_batch, training_label) == 1:\n",
    "        converged_batch += 1\n",
    "        if converged_batch % 10 == 0:\n",
    "            print(\"Total epochs passes: \", total_batches)\n",
    "            learner.printSummary()\n",
    "        \n",
    "    if total_batches % 50000 == 0:\n",
    "        print(\"Total epochs passes: \", total_batches)\n",
    "        learner.printSummary()\n",
    "        \n",
    "    if converged_batch > 20 or total_batches > 500000:\n",
    "        print(\"the learner has converged in \", total_batches)\n",
    "        learner.printSummary()    \n",
    "        break\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = np.array([[132.54], [-89.23]])\n",
    "coeff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(training_batch, coeff).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 5, (training_batch.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = generate_training_labels(training_batch)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CS6475]",
   "language": "python",
   "name": "conda-env-CS6475-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
