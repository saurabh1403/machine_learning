{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegressionLearner:\n",
    "    num_coeff = 1      # this doesn't include the bias term\n",
    "    weights = np.random.random(10)     #learnable weights\n",
    "    learnt_gamma = np.random.normal(0, 1)  # learnable parameter for thre batch norm\n",
    "    learnt_beta = np.random.normal(0, 1)  # learnable parameter for thre batch norm\n",
    "    learning_rate = 0.005\n",
    "    convergence_limit = 0.01\n",
    "    converged_batch = 0\n",
    "    converged_batch_limit = 50    # if this many batches are converged, then we can stop training\n",
    "    enable_batch_norm = False\n",
    "\n",
    "    def __init__(self, num_coeff, alpha, enable_batch_norm = False):\n",
    "        self.num_coeff = num_coeff\n",
    "        self.learning_rate = alpha\n",
    "        self.enable_batch_norm = enable_batch_norm\n",
    "        self.weights = np.random.normal(0, 1, (num_coeff + 1, 1))\n",
    "    \n",
    "    def printSummary(self):\n",
    "        print(\"The learnt weights are: \", self.weights)\n",
    "        print(\"The learnt gamma and beta are:\", self.learnt_gamma, \" and \", self.learnt_beta)\n",
    "\n",
    "    # return whether the batch converged or not\n",
    "    def learnGradient(self, training_batch, training_labels):\n",
    "        num_rows = training_batch.shape[0]\n",
    "\n",
    "        # few checks on the input\n",
    "        if training_batch.shape[1] != self.num_coeff:\n",
    "            print(\"Invalid number of columns: \", num_coeff)\n",
    "            return -1\n",
    "\n",
    "        # add the first column for bias learning\n",
    "#         print(\"training batch: \", training_batch)\n",
    "        bias_col = np.ones((num_rows, 1))\n",
    "        mod_train_batch = np.append(bias_col, training_batch, axis=1)\n",
    "#         print(\"modified training batch: \", mod_train_batch)\n",
    "\n",
    "        pred_label = np.matmul(mod_train_batch, self.weights)\n",
    "#         print(\"weights are:\", self.weights)\n",
    "#         print(\"predicted label is:\", pred_label)\n",
    "#         print(\"actual label is:\", training_label)\n",
    "        \n",
    "        pred_delta = pred_label - training_labels\n",
    "#         print(\"pred delta is:\", pred_delta)\n",
    "        \n",
    "        max_delta = 0\n",
    "        for i in range(self.num_coeff + 1):\n",
    "            col_feature = np.reshape(mod_train_batch[:, i], (num_rows, 1))\n",
    "#             print(\"col feature is:\", col_feature)\n",
    "            error_delta = pred_delta * col_feature\n",
    "#             print(\"err delta is:\", error_delta)\n",
    "            \n",
    "            dl_by_dw = np.mean(error_delta)\n",
    "#             dl_by_dw = min(dl_by_dw, 5)\n",
    "            self.weights[i] -= self.learning_rate * dl_by_dw\n",
    "            \n",
    "#             print(\"new weights are:\", self.weights)\n",
    "            max_delta = max(max_delta, np.abs(dl_by_dw))\n",
    "            \n",
    "        if max_delta <= self.convergence_limit:\n",
    "            self.converged_batch += 1\n",
    "            return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def learnGradientUsingBatchNorm(self, training_batch, training_labels):\n",
    "        num_rows = training_batch.shape[0]\n",
    "\n",
    "        # few checks on the input\n",
    "        if training_batch.shape[1] != self.num_coeff:\n",
    "            print(\"Invalid number of columns: \", num_coeff)\n",
    "            return -1\n",
    "\n",
    "#         print(\"modified training batch: \", mod_train_batch)\n",
    "\n",
    "        #standardize the input training data\n",
    "        mod_train_batch = (training_batch - np.mean(training_batch, axis=0))/(np.std(training_batch) + 0.01)\n",
    "\n",
    "        # add the first column for bias learning\n",
    "        bias_col = np.ones((num_rows, 1))\n",
    "        mod_train_batch = np.append(bias_col, mod_train_batch, axis=1)\n",
    "#         print(\"standardized training batch: \", mod_train_batch)\n",
    "\n",
    "        pred_label_1 = np.matmul(mod_train_batch, self.weights)\n",
    "        pred_label_2 = self.learnt_gamma * pred_label_1 + self.learnt_beta\n",
    "\n",
    "#         print(\"weights are:\", self.weights)\n",
    "#         print(\"predicted label is:\", pred_label)\n",
    "#         print(\"actual label is:\", training_label)\n",
    "\n",
    "        pred_delta_2 = pred_label_2 - training_labels\n",
    "#         print(\"pred delta is:\", pred_delta)\n",
    "\n",
    "        # update the weights\n",
    "        max_delta = 0\n",
    "        for i in range(self.num_coeff + 1):\n",
    "            col_feature = np.reshape(mod_train_batch[:, i], (num_rows, 1))\n",
    "#             print(\"col feature is:\", col_feature)\n",
    "            error_delta = pred_delta_2 * col_feature\n",
    "#             print(\"err delta is:\", error_delta)\n",
    "            error_delta *= self.learnt_gamma\n",
    "\n",
    "            dl_by_dw = np.mean(error_delta)\n",
    "#             dl_by_dw = min(dl_by_dw, 5)\n",
    "            self.weights[i] -= self.learning_rate * dl_by_dw\n",
    "\n",
    "#             print(\"new weights are:\", self.weights)\n",
    "            max_delta = max(max_delta, np.abs(dl_by_dw))\n",
    "\n",
    "        # update the learnable gamma and betas\n",
    "        dl_by_dgamma = pred_delta_2 * pred_label_1\n",
    "        self.learnt_gamma -= self.learning_rate * np.mean(dl_by_dgamma)\n",
    "\n",
    "        dl_by_dbeta = np.mean(pred_delta_2)\n",
    "        self.learnt_beta -= self.learning_rate * dl_by_dbeta\n",
    "\n",
    "        if max_delta <= self.convergence_limit:\n",
    "            self.converged_batch += 1\n",
    "            return 1\n",
    "\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epochs passes:  16\n",
      "The learnt weights are:  [[nan]\n",
      " [nan]]\n",
      "The learnt gamma and beta are: nan  and  nan\n",
      "Total epochs passes:  26\n",
      "The learnt weights are:  [[nan]\n",
      " [nan]]\n",
      "The learnt gamma and beta are: nan  and  nan\n",
      "the learner has converged in  27\n",
      "The learnt weights are:  [[nan]\n",
      " [nan]]\n",
      "The learnt gamma and beta are: nan  and  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/CS6475/lib/python3.5/site-packages/ipykernel/__main__.py:84: RuntimeWarning: overflow encountered in multiply\n",
      "/anaconda3/envs/CS6475/lib/python3.5/site-packages/numpy/core/_methods.py:75: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate the training data for a simple mathematical equation\n",
    "\n",
    "def generate_training_batch(num_coeff, batch_size):    # this doesn't include the bias term\n",
    "    # this is now generating features normalized around 0. \n",
    "    # We would play with it to see how batch norm can handle any feature distribution and scaling aspects\n",
    "    return np.random.normal(100, 50, (batch_size, num_coeff))\n",
    "\n",
    "\n",
    "def generate_training_labels(training_batch):\n",
    "    # y = -11.52 + 132.54 * x1 - 89.23 * x2\n",
    "    coeff = np.array([[132.54]])\n",
    "#     coeff = np.array([[132.54], [-89.23]])\n",
    "    label = np.matmul(training_batch, coeff)\n",
    "    label = label - 11.52\n",
    "    noise = np.random.normal(0, 5, (training_batch.shape[0], 1))\n",
    "    label += noise\n",
    "    return np.reshape(label, (training_batch.shape[0], 1))\n",
    "\n",
    "\n",
    "num_coeff = 1\n",
    "converged_batch = 0\n",
    "batch_size = 100\n",
    "total_batches = 0\n",
    "learner = LinRegressionLearner(num_coeff, 0.005)\n",
    "\n",
    "while(True):\n",
    "    training_batch = generate_training_batch(num_coeff=num_coeff, batch_size=batch_size)\n",
    "    training_label = generate_training_labels(training_batch)\n",
    "    total_batches += 1\n",
    "#     if learner.learnGradient(training_batch, training_label) == 1:\n",
    "    if learner.learnGradientUsingBatchNorm(training_batch, training_label) == 1:\n",
    "        converged_batch += 1\n",
    "        if converged_batch % 10 == 0:\n",
    "            print(\"Total epochs passes: \", total_batches)\n",
    "            learner.printSummary()\n",
    "        \n",
    "    if total_batches % 100000 == 0:\n",
    "        print(\"Total epochs passes: \", total_batches)\n",
    "        learner.printSummary()\n",
    "        \n",
    "    if converged_batch > 20: #or total_batches > 500000:\n",
    "        print(\"the learner has converged in \", total_batches)\n",
    "        learner.printSummary()    \n",
    "        break\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52206574],\n",
       "       [-0.23459001],\n",
       "       [-0.45171382],\n",
       "       [-0.76298217],\n",
       "       [ 1.97135175]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler().fit_transform(training_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52196138],\n",
       "       [-0.23454312],\n",
       "       [-0.45162352],\n",
       "       [-0.76282964],\n",
       "       [ 1.97095766]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training_batch - np.mean(training_batch, axis=0))/(np.std(training_batch) + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(training_batch, coeff).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 5, (training_batch.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = generate_training_labels(training_batch)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CS6475]",
   "language": "python",
   "name": "conda-env-CS6475-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
